{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will learn how to use a library for probabilistic programming and inference called <a href=\"http://docs.pymc.io/\">PyMC3</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "Libraries that are required for this tasks can be installed with the following command (if you use PyPI):\n",
    "\n",
    "```bash\n",
    "pip install pymc3 pandas numpy matplotlib seaborn\n",
    "```\n",
    "\n",
    "You can also install pymc3 from source using <a href=\"https://github.com/pymc-devs/pymc3#installation\">the instruction</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# set up session:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as rnd\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "from matplotlib import animation\n",
    "import pymc3 as pm\n",
    "from grader import Grader\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instance below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to the platform only after running submitting function in the last part of this assignment. If you want to make a partial submission, you can run that cell anytime you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Alice and Bob\n",
    "\n",
    "Alice and Bob are trading on the market. Both of them are selling the Thing and want to get as high profit as possible.\n",
    "Every hour they check out with each other's prices and adjust their prices to compete on the market. Although they have different strategies for price setting.\n",
    "\n",
    "**Alice**: takes Bob's price during the **previous** hour, multiply by 0.6, add 90\\$, add Gaussian noise from $N(0, 20^2)$.\n",
    "\n",
    "**Bob**: takes Alice's price during the **current** hour, multiply by 1.2 and subtract 20\\$, add Gaussian noise from $N(0, 10^2)$.\n",
    "\n",
    "The problem is to find the joint distribution of Alice and Bob's prices after many hours of such an experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "\n",
    "Implement the `run_simulation` function according to the description above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_simulation(\n",
    "    alice_start_price=300.0, \n",
    "    bob_start_price=300.0, \n",
    "    seed=42, \n",
    "    num_hours=10000, \n",
    "    burnin=1000\n",
    "):\n",
    "    \"\"\"Simulates an evolution of prices set by Bob and Alice.\n",
    "    \n",
    "    The function should simulate Alice and Bob behavior for `burnin' hours, then ignore the obtained\n",
    "    simulation results, and then simulate it for `num_hours' more.\n",
    "    The initial burnin (also sometimes called warmup) is done to make sure that the distribution stabilized.\n",
    "    \n",
    "    Please don't change the signature of the function.\n",
    "    \n",
    "    Returns:\n",
    "        two lists, with Alice and with Bob prices. Both lists should be of length num_hours.\n",
    "    \"\"\"\n",
    "    # set random seed for generator:\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # init price histories:\n",
    "    alice_prices = [alice_start_price]\n",
    "    bob_prices = [bob_start_price]\n",
    "    \n",
    "    # simulation:\n",
    "    for _ in range(num_hours):\n",
    "        '''\n",
    "        Alice: \n",
    "            take Bob's price during previous hour\n",
    "            multiply by 0.6 and add $90\n",
    "            then add Gaussian noise np.random.normal(loc=0.0, scale=20.0)\n",
    "        '''\n",
    "        alice_prices.append(\n",
    "            np.random.normal(\n",
    "                loc = 90 + 0.6*bob_prices[-1],\n",
    "                scale = 20.0\n",
    "            )\n",
    "        )\n",
    "        '''\n",
    "        Bob: \n",
    "            take Alice's price during current hour, \n",
    "            multiply by 1.2 and substract $20\n",
    "            then add Gaussian noise np.random.normal(loc=0.0, scale=10.0)\n",
    "        '''\n",
    "        bob_prices.append(\n",
    "            np.random.normal(\n",
    "                loc = -20 + 1.2*alice_prices[-1],\n",
    "                scale = 10.0\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # return stationary samples:\n",
    "    return alice_prices[burnin:], bob_prices[burnin:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task 1.1 (Alice trajectory) is: 279.93428306022463  291.67686875834846\n",
      "Current answer for task 1.1 (Bob trajectory) is: 314.5384966605577  345.2425410740984\n"
     ]
    }
   ],
   "source": [
    "alice_prices, bob_prices = run_simulation(\n",
    "    alice_start_price=300, \n",
    "    bob_start_price=300, \n",
    "    seed=42, \n",
    "    num_hours=3, \n",
    "    burnin=1\n",
    ")\n",
    "\n",
    "if len(alice_prices) != 3:\n",
    "    raise RuntimeError('Make sure that the function returns `num_hours` data points.')\n",
    "\n",
    "grader.submit_simulation_trajectory(alice_prices, bob_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2\n",
    "What is the average prices for Alice and Bob after the burnin period? Whose prices are higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run simulation again:\n",
    "alice_prices, bob_prices = run_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task 1.2 (Alice mean) is: 278.628212955\n",
      "Current answer for task 1.2 (Bob mean) is: 314.376422877\n"
     ]
    }
   ],
   "source": [
    "# average price:\n",
    "average_alice_price = np.mean(alice_prices)\n",
    "average_bob_price = np.mean(bob_prices)\n",
    "\n",
    "grader.submit_simulation_mean(average_alice_price, average_bob_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3\n",
    "\n",
    "Let's look at the 2-d histogram of prices, computed using kernel density estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.JointGrid at 0x7f5e89358be0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAasAAAGoCAYAAAD4hcrDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e89358c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.array(\n",
    "    run_simulation()\n",
    ")\n",
    "\n",
    "sns.jointplot(data[0, :], data[1, :], stat_func=None, kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the prices of Bob and Alce are highly correlated. What is the Pearson correlation coefficient of Alice and Bob prices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task 1.3 (Bob and Alice prices correlation) is: 0.963634002516\n"
     ]
    }
   ],
   "source": [
    "# Pearson correlation coefficient:\n",
    "(correlation, p_value) = pearsonr(data[0], data[1])\n",
    "\n",
    "grader.submit_simulation_correlation(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting effect here: seems like the bivariate distribution of Alice and Bob prices converges to a correlated bivariate Gaussian distribution.\n",
    "\n",
    "Let's check, whether the results change if we use different random seed and starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 16\n",
      "[Mean]:\n",
      " [ 279.35926979  315.20824249]\n",
      "[Cov]:\n",
      " [[  922.23500838  1111.82933481]\n",
      " [ 1111.82933481  1438.75644842]]\n",
      "\n",
      "10 19\n",
      "[Mean]:\n",
      " [ 278.73206432  314.51921767]\n",
      "[Cov]:\n",
      " [[  901.3615779   1082.48709653]\n",
      " [ 1082.48709653  1398.1673765 ]]\n",
      "\n",
      "10 42\n",
      "[Mean]:\n",
      " [ 278.62821295  314.37642288]\n",
      "[Cov]:\n",
      " [[  889.92775486  1065.69561517]\n",
      " [ 1065.69561517  1374.31844124]]\n",
      "\n",
      "10 73\n",
      "[Mean]:\n",
      " [ 277.52400056  313.14102543]\n",
      "[Cov]:\n",
      " [[  916.23009585  1098.6416554 ]\n",
      " [ 1098.6416554   1415.41723888]]\n",
      "\n",
      "100 16\n",
      "[Mean]:\n",
      " [ 279.35926979  315.20824249]\n",
      "[Cov]:\n",
      " [[  922.23500838  1111.82933481]\n",
      " [ 1111.82933481  1438.75644842]]\n",
      "\n",
      "100 19\n",
      "[Mean]:\n",
      " [ 278.73206432  314.51921767]\n",
      "[Cov]:\n",
      " [[  901.3615779   1082.48709653]\n",
      " [ 1082.48709653  1398.1673765 ]]\n",
      "\n",
      "100 42\n",
      "[Mean]:\n",
      " [ 278.62821295  314.37642288]\n",
      "[Cov]:\n",
      " [[  889.92775486  1065.69561517]\n",
      " [ 1065.69561517  1374.31844124]]\n",
      "\n",
      "100 73\n",
      "[Mean]:\n",
      " [ 277.52400056  313.14102543]\n",
      "[Cov]:\n",
      " [[  916.23009585  1098.6416554 ]\n",
      " [ 1098.6416554   1415.41723888]]\n",
      "\n",
      "1000 16\n",
      "[Mean]:\n",
      " [ 279.35926979  315.20824249]\n",
      "[Cov]:\n",
      " [[  922.23500838  1111.82933481]\n",
      " [ 1111.82933481  1438.75644842]]\n",
      "\n",
      "1000 19\n",
      "[Mean]:\n",
      " [ 278.73206432  314.51921767]\n",
      "[Cov]:\n",
      " [[  901.3615779   1082.48709653]\n",
      " [ 1082.48709653  1398.1673765 ]]\n",
      "\n",
      "1000 42\n",
      "[Mean]:\n",
      " [ 278.62821295  314.37642288]\n",
      "[Cov]:\n",
      " [[  889.92775486  1065.69561517]\n",
      " [ 1065.69561517  1374.31844124]]\n",
      "\n",
      "1000 73\n",
      "[Mean]:\n",
      " [ 277.52400056  313.14102543]\n",
      "[Cov]:\n",
      " [[  916.23009585  1098.6416554 ]\n",
      " [ 1098.6416554   1415.41723888]]\n",
      "\n",
      "10000 16\n",
      "[Mean]:\n",
      " [ 279.35926979  315.20824249]\n",
      "[Cov]:\n",
      " [[  922.23500838  1111.82933481]\n",
      " [ 1111.82933481  1438.75644842]]\n",
      "\n",
      "10000 19\n",
      "[Mean]:\n",
      " [ 278.73206432  314.51921767]\n",
      "[Cov]:\n",
      " [[  901.3615779   1082.48709653]\n",
      " [ 1082.48709653  1398.1673765 ]]\n",
      "\n",
      "10000 42\n",
      "[Mean]:\n",
      " [ 278.62821295  314.37642288]\n",
      "[Cov]:\n",
      " [[  889.92775486  1065.69561517]\n",
      " [ 1065.69561517  1374.31844124]]\n",
      "\n",
      "10000 73\n",
      "[Mean]:\n",
      " [ 277.52400056  313.14102543]\n",
      "[Cov]:\n",
      " [[  916.23009585  1098.6416554 ]\n",
      " [ 1098.6416554   1415.41723888]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for starting_price in (10, 100, 1000, 10000):\n",
    "    for seed in (16, 19, 42, 73):\n",
    "        # generate stationary distribution:\n",
    "        data = run_simulation(\n",
    "            alice_start_price=starting_price, \n",
    "            bob_start_price=starting_price, \n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        # distribution params:\n",
    "        mu = np.mean(data, axis = 1)\n",
    "        cov = np.cov(data)\n",
    "        \n",
    "        # display params:\n",
    "        print(starting_price, seed)\n",
    "        print(\"[Mean]:\\n\", mu)\n",
    "        print(\"[Cov]:\\n\", cov)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task 1.4 (depends on the random data or not) is: Does not depend on random seed and starting prices\n"
     ]
    }
   ],
   "source": [
    "# Pick different starting prices, e.g 10, 1000, 10000 for Bob and Alice. \n",
    "# Does the joint distribution of the two prices depend on these parameters?\n",
    "POSSIBLE_ANSWERS = {\n",
    "    0: 'Depends on random seed and starting prices', \n",
    "    1: 'Depends only on random seed',\n",
    "    2: 'Depends only on starting prices',\n",
    "    3: 'Does not depend on random seed and starting prices'\n",
    "}\n",
    "\n",
    "idx = 3\n",
    "answer = POSSIBLE_ANSWERS[idx]\n",
    "grader.submit_simulation_depends(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 2. Logistic regression with PyMC3\n",
    "\n",
    "Logistic regression is a powerful model that allows you to analyze how a set of features affects some binary target label. Posterior distribution over the weights gives us an estimation of the influence of each particular feature on the probability of the target being equal to one. But most importantly, posterior distribution gives us the interval estimates for each weight of the model. This is very important for data analysis when you want to not only provide a good model but also estimate the uncertainty of your conclusions.\n",
    "\n",
    "In this task, we will learn how to use PyMC3 library to perform approximate Bayesian inference for logistic regression.\n",
    "\n",
    "This part of the assignment is based on the logistic regression tutorial by Peadar Coyle and J. Benjamin Cook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression.\n",
    "\n",
    "The problem here is to model how the probability that a person has salary $\\geq$ \\$50K is affected by his/her age, education, sex and other features.\n",
    "\n",
    "Let $y_i = 1$ if i-th person's salary is $\\geq$ \\$50K and $y_i = 0$ otherwise. Let $x_{ij}$ be $j$-th feature of $i$-th person.\n",
    "\n",
    "Logistic regression models this probabilty in the following way:\n",
    "\n",
    "$$p(y_i = 1 \\mid \\beta) = \\sigma (\\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_k x_{ik} ), $$\n",
    "\n",
    "where $\\sigma(t) = \\frac1{1 + e^{-t}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Odds ratio.\n",
    "Let's try to answer the following question: does a gender of a person affects his or her salary? To do it we will use the concept of *odds*.\n",
    "\n",
    "If we have a binary random variable $y$ (which may indicate whether a person makes \\$50K) and if the probabilty of the positive outcome $p(y = 1)$ is for example 0.8, we will say that the *odds* are 4 to 1 (or just 4 for short), because succeding is 4 time more likely than failing $\\frac{p(y = 1)}{p(y = 0)} = \\frac{0.8}{0.2} = 4$.\n",
    "\n",
    "Now, let's return to the effect of gender on the salary. Let's compute the **ratio** between the odds of a male having salary $\\geq $ \\$50K and the odds of a female (with the same level of education, experience and everything else) having salary $\\geq$ \\$50K. The first feature of each person in the dataset is the gender. Specifically, $x_{i1} = 0$ if the person is female and $x_{i1} = 1$ otherwise. Consider two people $i$ and $j$ having all but one features the same with the only difference in $x_{i1} \\neq x_{j1}$.\n",
    "\n",
    "If the logistic regression model above estimates the probabilities exactly, the odds for a male will be (check it!):\n",
    "$$\n",
    "\\frac{p(y_i = 1 \\mid x_{i1}=1, x_{i2}, \\ldots, x_{ik})}{p(y_i = 0 \\mid x_{i1}=1, x_{i2}, \\ldots, x_{ik})} = \\frac{\\sigma(\\beta_1 + \\beta_2 x_{i2} + \\ldots)}{1 - \\sigma(\\beta_1 + \\beta_2 x_{i2} + \\ldots)} = \\exp(\\beta_1 + \\beta_2 x_{i2} + \\ldots)\n",
    "$$\n",
    "\n",
    "Now the ratio of the male and female odds will be:\n",
    "$$\n",
    "\\frac{\\exp(\\beta_1 \\cdot 1 + \\beta_2 x_{i2} + \\ldots)}{\\exp(\\beta_1 \\cdot 0 + \\beta_2 x_{i2} + \\ldots)} = \\exp(\\beta_1)\n",
    "$$\n",
    "\n",
    "So given the correct logistic regression model, we can estimate odds ratio for some feature (gender in this example) by just looking at the corresponding coefficient. But of course, even if all the logistic regression assumptions are met we cannot estimate the coefficient exactly from real-world data, it's just too noisy. So it would be really nice to build an interval estimate, which would tell us something along the lines \"with probability 0.95 the odds ratio is greater than 0.8 and less than 1.2, so we cannot conclude that there is any gender discrimination in the salaries\" (or vice versa, that \"with probability 0.95 the odds ratio is greater than 1.5 and less than 1.9 and the discrimination takes place because a male has at least 1.5 higher probability to get >$50k than a female with the same level of education, age, etc.\"). In Bayesian statistics, this interval estimate is called *credible interval*.\n",
    "\n",
    "Unfortunately, it's impossible to compute this credible interval analytically. So let's use MCMC for that!\n",
    "\n",
    "#### Credible interval\n",
    "A credible interval for the value of $\\exp(\\beta_1)$ is an interval $[a, b]$ such that $p(a \\leq \\exp(\\beta_1) \\leq b \\mid X_{\\text{train}}, y_{\\text{train}})$ is $0.95$ (or some other predefined value). To compute the interval, we need access to the posterior distribution $p(\\exp(\\beta_1) \\mid X_{\\text{train}}, y_{\\text{train}})$.\n",
    "\n",
    "Lets for simplicity focus on the posterior on the parameters $p(\\beta_1 \\mid X_{\\text{train}}, y_{\\text{train}})$ since if we compute it, we can always find $[a, b]$ such that $p(\\log a \\leq \\beta_1 \\leq \\log b \\mid X_{\\text{train}}, y_{\\text{train}}) = p(a \\leq \\exp(\\beta_1) \\leq b \\mid X_{\\text{train}}, y_{\\text{train}}) = 0.95$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 MAP inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the dataset. This is a post-processed version of the [UCI Adult dataset](http://archive.ics.uci.edu/ml/datasets/Adult)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>hours</th>\n",
       "      <th>income_more_50K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sex  age  educ  hours  income_more_50K\n",
       "0     Male   39    13     40                0\n",
       "1     Male   50    13     13                0\n",
       "2     Male   38     9     40                0\n",
       "3     Male   53     7     40                0\n",
       "4   Female   28    13     40                0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"adult_us_postprocessed.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add feature:\n",
    "data['age_squared'] = data['age']**2\n",
    "data['sex'] = data['sex'].astype(np.str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the dataset is a person with his (her) features. The last column is the target variable $y$. 1 indicates that this person's annual salary is more than $50K.\n",
    "\n",
    "First of all let's set up a Bayesian logistic regression model (i.e. define priors on the parameters $\\alpha$ and $\\beta$ of the model) that predicts the value of \"income_more_50K\" based on person's age and education:\n",
    "\n",
    "$$\n",
    "p(y = 1 \\mid \\alpha, \\beta_1, \\beta_2) = \\sigma(\\alpha + \\beta_1 x_1 + \\beta_2 x_2) \\\\ \n",
    "\\alpha \\sim N(0, 100^2) \\\\\n",
    "\\beta_1 \\sim N(0, 100^2) \\\\\n",
    "\\beta_2 \\sim N(0, 100^2), \\\\\n",
    "$$\n",
    "\n",
    "where $x_1$ is a person's age, $x_2$ is his/her level of education, y indicates his/her level of income, $\\alpha$, $\\beta_1$ and $\\beta_2$ are paramters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logp = -18,844, ||grad|| = 57,293: 100%|██████████| 30/30 [00:00<00:00, 132.53it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': array(-6.748119036452204), 'beta1': array(0.04348316196411672), 'beta2': array(0.3621080327555488)}\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as manual_logistic_model:\n",
    "    # Declare pymc random variables for logistic regression coefficients with uninformative \n",
    "    # prior distributions N(0, 100^2) on each weight using pm.Normal. \n",
    "    # Don't forget to give each variable a unique name.\n",
    "    alpha = pm.Normal('alpha', 0, 100)\n",
    "    beta1 = pm.Normal('beta1', 0, 100)\n",
    "    beta2 = pm.Normal('beta2', 0, 100)\n",
    "    # Thansform these random variables into vector of probabilities p(y_i=1) using logistic regression model specified \n",
    "    # above. PyMC random variables are theano shared variables and support simple mathematical operations.\n",
    "    # For example:\n",
    "    # z = pm.Normal('x', 0, 1) * np.array([1, 2, 3]) + pm.Normal('y', 0, 1) * np.array([4, 5, 6])`\n",
    "    # is a correct PyMC expression.\n",
    "    # Use pm.invlogit for the sigmoid function.\n",
    "    z = alpha + beta1*data['age'].values + beta2*data['educ'].values\n",
    "    p = pm.invlogit(z)\n",
    "    # Declare PyMC Bernoulli random vector with probability of success equal to the corresponding value\n",
    "    # given by the sigmoid function.\n",
    "    # Supply target vector using \"observed\" argument in the constructor.\n",
    "    L = pm.Bernoulli('L', p, observed=data['income_more_50K'].values)\n",
    "    # Use pm.find_MAP() to find the maximum a-posteriori estimate for the vector of logistic regression weights.\n",
    "    map_estimate = pm.find_MAP()\n",
    "    print(map_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sumbit MAP estimations of corresponding coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logp = -15,131, ||grad|| = 0.024014: 100%|██████████| 32/32 [00:00<00:00, 209.86it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'educ': array(0.3621089416949504), 'age': array(0.04348258952614433), 'Intercept': array(-6.748099804701474)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as logistic_model:\n",
    "    # There's a simpler interface for generalized linear models in pymc3. \n",
    "    # Try to train the same model using pm.glm.GLM.from_formula.\n",
    "    # Do not forget to specify that the target variable is binary (and hence follows Binomial distribution).\n",
    "    pm.glm.GLM.from_formula(\n",
    "        'income_more_50K ~ age + educ', \n",
    "        data, \n",
    "        family=pm.glm.families.Binomial()\n",
    "    )\n",
    "    \n",
    "    map_estimate = pm.find_MAP(\n",
    "        model = logistic_model\n",
    "    )\n",
    "    print(map_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task 2.1 (MAP for age coef) is: 0.04348258952614433\n",
      "Current answer for task 2.1 (MAP for aducation coef) is: 0.3621089416949504\n"
     ]
    }
   ],
   "source": [
    "beta_age_coefficient = 0.04348258952614433\n",
    "beta_education_coefficient = 0.3621089416949504\n",
    "grader.submit_pymc_map_estimates(beta_age_coefficient, beta_education_coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find credible regions let's perform MCMC inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You will need the following function to visualize the sampling process.\n",
    "# You don't need to change it.\n",
    "def plot_traces(traces, burnin=2000):\n",
    "    ''' \n",
    "    Convenience function:\n",
    "    Plot traces with overlaid means and values\n",
    "    '''\n",
    "    ax = pm.traceplot(\n",
    "        traces[burnin:], \n",
    "        figsize=(12,len(traces.varnames)*1.5),\n",
    "        lines={k: v['mean'] for k, v in pm.df_summary(traces[burnin:]).iterrows()}\n",
    "    )\n",
    "\n",
    "    for i, mn in enumerate(pm.df_summary(traces[burnin:])['mean']):\n",
    "        ax[i,0].annotate('{:.2f}'.format(mn), xy=(mn,0), xycoords='data'\n",
    "                    ,xytext=(5,10), textcoords='offset points', rotation=90\n",
    "                    ,va='bottom', fontsize='large', color='#AA0022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metropolis-Hastings\n",
    "Let's use Metropolis-Hastings algorithm for finding the samples from the posterior distribution.\n",
    "\n",
    "Once you wrote the code, explore the hyperparameters of Metropolis-Hastings such as the proposal distribution variance to speed up the convergence. You can use `plot_traces` function in the next cell to visually inspect the convergence.\n",
    "\n",
    "You may also use MAP-estimate to initialize the sampling scheme to speed things up. This will make the warmup (burnin) period shorter since you will start from a probable point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [hours]\n",
      ">Metropolis: [educ]\n",
      ">Metropolis: [age_squared]\n",
      ">Metropolis: [age]\n",
      ">Metropolis: [sex[T. Male]]\n",
      ">Metropolis: [Intercept]\n",
      " 87%|████████▋ | 782/900 [00:34<00:05, 22.85it/s]"
     ]
    }
   ],
   "source": [
    "with pm.Model() as logistic_model:\n",
    "    # Since it is unlikely that the dependency between the age and salary is linear\n",
    "    # we will include age squared into features so that we can model dependency that favors certain ages.\n",
    "    # Train Bayesian logistic regression model on the following features: sex, age, age_squared, educ, hours\n",
    "    # Use pm.sample to run MCMC to train this model.\n",
    "    # To specify the particular sampler method (Metropolis-Hastings) to pm.sample,\n",
    "    # use `pm.Metropolis`.\n",
    "    # Train your model for 400 samples.\n",
    "    # Save the output of pm.sample to a variable: this is the trace of the sampling procedure and will be used\n",
    "    # to estimate the statistics of the posterior distribution.\n",
    "    \n",
    "    # define model:\n",
    "    pm.glm.GLM.from_formula(\n",
    "        'income_more_50K ~ sex + age + age_squared + educ + hours', \n",
    "        data, \n",
    "        family=pm.glm.families.Binomial()\n",
    "    )\n",
    "    # sampling method:\n",
    "    step = pm.Metropolis()\n",
    "    # sample:\n",
    "    trace = pm.sample(\n",
    "        400, chains=1, step = step\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_traces(trace, burnin=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NUTS sampler\n",
    "Use pm.sample without specifying a particular sampling method (pymc3 will choose it automatically).\n",
    "The sampling algorithm that will be used in this case is NUTS, which is a form of Hamiltonian Monte Carlo, in which parameters are tuned automatically. This is an advanced method that we hadn't cover in the lectures, but it usually converges faster and gives less correlated samples compared to vanilla Metropolis-Hastings.\n",
    "\n",
    "Since the NUTS sampler doesn't require to tune hyperparameters, let's run it for 10 times more iterations than Metropolis-Hastings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pm.Model() as logistic_model:\n",
    "    # Train Bayesian logistic regression model on the following features: sex, age, age_squared, educ, hours\n",
    "    # Use pm.sample to run MCMC to train this model.\n",
    "    # Train your model for *4000* samples (ten times more than before).\n",
    "    # Training can take a while, so relax and wait :)\n",
    "    \n",
    "    # define model:\n",
    "    pm.glm.GLM.from_formula(\n",
    "        'income_more_50K ~ sex + age + age_squared + educ + hours', \n",
    "        data, \n",
    "        family=pm.glm.families.Binomial()\n",
    "    )\n",
    "    # sampling method -- Hamiltonian Monte Carlo:\n",
    "    # sample:\n",
    "    trace = pm.sample(4000, chains=1, tune=1000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_traces(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the odds ratio\n",
    "Now, let's build the posterior distribution on the odds ratio given the dataset (approximated by MCMC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We don't need to use a large burn-in here, since we initialize sampling\n",
    "# from a good point (from our approximation of the most probable\n",
    "# point (MAP) to be more precise).\n",
    "burnin = 100\n",
    "b = trace['sex[T. Male]'][burnin:]\n",
    "plt.hist(np.exp(b), bins=20, normed=True)\n",
    "plt.xlabel(\"Odds Ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we can find a credible interval  (recall that credible intervals are Bayesian and confidence intervals are frequentist) for this quantity. This may be the best part about Bayesian statistics: we get to interpret credibility intervals the way we've always wanted to interpret them. We are 95% confident that the odds ratio lies within our interval!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb, ub = np.percentile(b, 2.5), np.percentile(b, 97.5)\n",
    "print(\"P(%.3f < Odds Ratio < %.3f) = 0.95\" % (np.exp(lb), np.exp(ub)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Submit the obtained credible interval.\n",
    "grader.submit_pymc_odds_ratio_interval(np.exp(lb), np.exp(ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 interpreting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Does the gender affects salary in the provided dataset?\n",
    "# (Note that the data is from 1996 and maybe not representative\n",
    "# of the current situation in the world.)\n",
    "POSSIBLE_ANSWERS = {\n",
    "    0: 'No, there is certainly no discrimination',\n",
    "    1: 'We cannot say for sure',\n",
    "    2: 'Yes, we are 95% sure that a female is *less* likely to get >$50K than a male with the same age, level of education, etc.', \n",
    "    3: 'Yes, we are 95% sure that a female is *more* likely to get >$50K than a male with the same age, level of education, etc.', \n",
    "}\n",
    "\n",
    "idx = 2\n",
    "answer = POSSIBLE_ANSWERS[idx]\n",
    "grader.submit_is_there_discrimination(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STUDENT_EMAIL = '937570601@qq.com'\n",
    "STUDENT_TOKEN = 'HfcAmpfz6oF6AEnw'\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) generating videos of sampling process\n",
    "For this (optional) part you will need to install ffmpeg, e.g. by the following command on linux\n",
    "\n",
    "    apt-get install ffmpeg\n",
    "\n",
    "or the following command on Mac\n",
    "\n",
    "    brew install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting things up\n",
    "You don't need to modify the code below, it sets up the plotting functions. The code is based on [MCMC visualization tutorial](https://twiecki.github.io/blog/2014/01/02/visualizing-mcmc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Number of MCMC iteration to animate.\n",
    "samples = 400\n",
    "\n",
    "figsize(6, 6)\n",
    "fig = plt.figure()\n",
    "s_width = (0.81, 1.29)\n",
    "a_width = (0.11, 0.39)\n",
    "samples_width = (0, samples)\n",
    "ax1 = fig.add_subplot(221, xlim=s_width, ylim=samples_width)\n",
    "ax2 = fig.add_subplot(224, xlim=samples_width, ylim=a_width)\n",
    "ax3 = fig.add_subplot(223, xlim=s_width, ylim=a_width,\n",
    "                      xlabel='male coef',\n",
    "                      ylabel='educ coef')\n",
    "fig.subplots_adjust(wspace=0.0, hspace=0.0)\n",
    "line1, = ax1.plot([], [], lw=1)\n",
    "line2, = ax2.plot([], [], lw=1)\n",
    "line3, = ax3.plot([], [], 'o', lw=2, alpha=.1)\n",
    "line4, = ax3.plot([], [], lw=1, alpha=.3)\n",
    "line5, = ax3.plot([], [], 'k', lw=1)\n",
    "line6, = ax3.plot([], [], 'k', lw=1)\n",
    "ax1.set_xticklabels([])\n",
    "ax2.set_yticklabels([])\n",
    "lines = [line1, line2, line3, line4, line5, line6]\n",
    "\n",
    "def init():\n",
    "    for line in lines:\n",
    "        line.set_data([], [])\n",
    "    return lines\n",
    "\n",
    "def animate(i):\n",
    "    with logistic_model:\n",
    "        if i == 0:\n",
    "            # Burnin\n",
    "            for j in range(samples): iter_sample.__next__() \n",
    "        trace = iter_sample.__next__()\n",
    "#     import pdb; pdb.set_trace()\n",
    "    line1.set_data(trace['sex[T. Male]'][::-1], range(len(trace['sex[T. Male]'])))\n",
    "    line2.set_data(range(len(trace['educ'])), trace['educ'][::-1])\n",
    "    line3.set_data(trace['sex[T. Male]'], trace['educ'])\n",
    "    line4.set_data(trace['sex[T. Male]'], trace['educ'])\n",
    "    male = trace['sex[T. Male]'][-1]\n",
    "    educ = trace['educ'][-1]\n",
    "    line5.set_data([male, male], [educ, a_width[1]])\n",
    "    line6.set_data([male, s_width[1]], [educ, educ])\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animating Metropolis-Hastings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pm.Model() as logistic_model:\n",
    "    # Again define Bayesian logistic regression model on the following features: sex, age, age_squared, educ, hours\n",
    "    \n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    ### END OF YOUR CODE ###\n",
    "    step = pm.Metropolis()\n",
    "    iter_sample = pm.iter_sample(2 * samples, step, start=map_estimate)\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=samples, interval=5, blit=True)\n",
    "HTML(anim.to_html5_video())\n",
    "# Note that generating the video may take a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animating NUTS\n",
    "Now rerun the animation providing the NUTS sampling method as the step argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
